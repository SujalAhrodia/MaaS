sudo ip netns exec pns ip tunnel add gre1 mode gre remote 172.19.0.5 local 172.19.0.3 ttl 64

sudo ip netns exec pns ip link set dev gre1 up

sudo ip netns exec pns ip addr add 172.18.0.1/24 dev gre1

ece792@t11_vm2:~$ sudo ip netns exec pns ip route
20.0.0.0/24 dev pns-00 proto kernel scope link src 20.0.0.2
172.18.0.0/24 dev gre1 proto kernel scope link src 172.18.0.1
172.19.0.0/24 dev pns-global proto kernel scope link src 172.19.0.3
172.19.0.5 via 172.19.0.4 dev pns-global

ece792@t11_vm2:~$ ip route
default via 192.168.126.1 dev ens6 proto dhcp src 192.168.126.90 metric 100
default via 192.168.124.1 dev ens5 proto dhcp src 192.168.124.85 metric 100
default via 192.168.122.1 dev ens3 proto dhcp src 192.168.122.8 metric 100
10.0.0.0/24 dev vxlan0 proto kernel scope link src 10.0.0.1
172.16.3.0/24 dev ens4 proto kernel scope link src 172.16.3.1
172.19.0.0/24 dev global-pns proto kernel scope link src 172.19.0.4
172.19.0.5 via 172.16.3.2 dev ens4
172.19.0.6 via 172.16.3.2 dev ens4
192.168.122.0/24 dev ens3 proto kernel scope link src 192.168.122.8
192.168.122.1 dev ens3 proto dhcp scope link src 192.168.122.8 metric 100
192.168.123.0/24 dev virbr0 proto kernel scope link src 192.168.123.1
192.168.124.0/24 dev ens5 proto kernel scope link src 192.168.124.85
192.168.124.1 dev ens5 proto dhcp scope link src 192.168.124.85 metric 100
192.168.126.0/24 dev ens6 proto kernel scope link src 192.168.126.90
192.168.126.1 dev ens6 proto dhcp scope link src 192.168.126.90 metric 100


sudo ip netns exec pns ip route add 21.0.0.0/24 dev gre1 via 172.18.0.1
sudo ip netns exec t1 ip route add default dev pns-t1 via 20.0.0.2
   

DEMO COMMAND:

curl -G http://192.168.123.95:8086/query?pretty=true --data-urlencode "db=collectd" --data-urlencode "q=SELECT \"value\" FROM \"ret_pol_1\".\"cpu_value\" LIMIT 10"

sudo ip netns exec t1sn1 ssh root@35.0.0.2 'ping 35.0.0.3 -c 20' | sudo ip netns exec pns tcpdump -i pns-global -nnev -c 10


Virt-builder commands

export LIBGUESTFS_BACKEND=direct
sudo virt-builder centos-7.0 --root-password password:root -o build-test.img --size 10G --firstboot-command 'dhclient' -v --attach <Centos ISO>

http://libguestfs.org/download/builder/centos-7.0.xz

virt-install -n temporary_vm --memory 2048 --vcpus 1 --import build-test.img --os-variant rhel7 

# Part 1 -> Create the network for a new tenant (in both hosts)
# Note: Subnet Router will require MASQUERADE rules (to be tested)
# Zone 1 -> 172.16.3.1
#   - Subnet Router range => 20.0.0.0/30 with lower IP at the pns-sr interface
#   - Tenant subnet range => 22.0.0.0/30 with lower IP at the sr-t<int> interface
#   - PNS to Host range => 172.19.0.0/30
#   - GRE interface in PNS => 172.18.0.1/24
# Zone 2 -> 172.16.3.2
#   - Subnet Router range => 21.0.0.0/30 with lower IP at the pns-sr interface
#   - Tenant subnet range => 23.0.0.0/30 with lower IP at the sr-t<int> interface
#   - PNS to Host range => 172.19.0.4/30
#   - GRE interface in PNS => 172.18.0.2/24
#   Create a subnet router
#   - Veth pair between subnet router and Provider Network
#   - Default route from subnet router to Provider Network
#   Create a tenant namespace (for that vnet)
#   - A bridge inside that namespace
#   - A VXLAN interface on that bridge
#   - The VXLAN tunnel endpoint will be the interface between the tenant NS and the Subnet Router
#   - A gateway from that bridge to the tenant namespace - Inside the User provided subnet
#   - Veth pair between subnet router and Tenant Network (per VNET)
#   - Route for tenant VNET on the subnet router to Tenant Netwark (per VNET)
#   - Add MASQUERADE rules on the subnet router for the Tenant Network
# Users will provide
#   Tenant name - Part 1
#   IP addresses for the VNET - Part 1
#   Number of hosts needed - Part 2
#   Zone the hosts need to be in - Part 2
#
# Part 2 -> Create VM in the zone requested
#   Spawn the VMs
#   Spawn bridges (libvirt bridges in bridge mode) per VM spawned
#   Connect the VM to the bridge
#   Add a veth pair from the bridge to the tenant VXLAN bridge
#   Add a default route on the VM
#   NOTE: Store VMs MAC address -> Use that python script from HW3 if we want to 
#         add per VM entry on the bridge fdb. We should test with default MAC entries
#         and verify the behaviour
#####~~NAMING CONVENTION~~#####
VM names = t#vm#
VM bridge name (libvirt bridge)= t#vm#br
VM bridge network name (libvirt network)= t#vm#net

Tenant namespace name = t#sn#
Tenant namespace bridge = t#sn#br
Interface between t#vm#br and t#sn#br = t#vm#br-t#sn#br and peer name t#sn#br-t#vm#br
VXLAN Interface = t#sn#vxlan<zone#>
Gateway between t#sn#br and t#sn# = t#sn#-t#sn#br and peer name t#sn#br-t#sn#

Subnet router name = t#sr
Interface between t#sr and t#sn# = t#sr-t#sn# and peer name t#sn#-t#sr#
Interface between t#sr and pns = t#sr-pns and peer name pns-t#sr
----Ansible ends here---

Interface between pns and host = host-pns and peer name pns-host

Step 1 to execute:
Mapping between User name and Tenant ID => There should exist a t#sr
Mapping between User VNET and Subnet ID => There should exist a t#sn#



###~~~Routes to add~~~###
sudo ip netns exec t#sr ip route add default via 20.0.0.1 dev t#sr-pns


The serial console is not working in this image (apparently a bug
in Ubuntu).  To enable it, do:

--edit '/etc/default/grub:
           s/^GRUB_CMDLINE_LINUX_DEFAULT=.*/GRUB_CMDLINE_LINUX_DEFAULT="console=tty0 console=ttyS0,115200n8"/' \
--run-command update-grub



Northbound for VPC deployment:
CLI for Users to generate their data in the format below
    Customer responsibility is to enter the information correctly.
    ? Maybe they will have an option to remove networks (using a ! identifier)

Data format =>

TenantName: {
    VPC: True,
    Zone1:{
        VM1: [
            10.0.0.0/24,
            ,
            sn3
        ]
        VM2: [
            sn3
        ]
    },
    Zone2: {
        VM3: [
            sn2
        ]
    }
}

TenantName:{
    Monitoring: True,
    CPU: True,
    Memory: True,
    Custom:
    {
    key: value,
    file: filename
    }
}

Logic Layer for VPC deployment:

TenantName-ID.json
{
    {   # Generated by logic layer
        'Subnets': {
            'IP': 'sn1',
            'IP': 'sn2'
        },
        VM1: [Zone, Management IP, IP1, IP2]
        IFDB: IP
        IFDB_bak: IP
        Virtual IP: IP
    },
    {   # Generated by the frontend
        flag: True,
        CPU: True,
        Memory: True,
        Custom:
        {
        flag: Tru,
        file: filename
        }
    }
}


{
    Zone1: 172.16.3.1
    Zone2: 172.16.3.2
    zone1_SR: 20.0.0
    zone2_SR: 21.0.0
    zone1_TR: 22.0.0
    zone2_TR: 23.0.0
    zone1_TR_subnet: 1
    zone2_TR_subnet: 254
}



Logic Layer 1:
    Generate TID from Tenant Name
        - Read Directory
        - If Tenant Name is already there:
            TID = Count of number of files + 1
            Parse and generate the file from teh data given by the User
        - Else:
            edit the same file
            Parse and add the data from the User

Logic Layer 2(in order):
    Check if subnet router exists: (ip netns list)
        If yes, continue
        Else, 
            use SR play - ansible-playbook --extra-vars "tenant.json" (#TID)
            use IFDB play - 
                update tenant.json with IFDB IPs and VIP chosen
                edit keepalived and start the service
                edit the backup.sh
    Check if TR exists: (ip netns list)
        If yes, continue
        Else, use TR play (#TID, #SID)
    Check if VM exists: (virsh list)
        If yes, continue
        Else, 
            use VM play (#TID,#VMID, #ZONE)
            edit tenant.json with VM Management IP
            edit collect.d conf and start the service
    Check if interface exists: (virsh domiflist)
        If yes, continue
        Else, use Interface play (#TID, #VMID, #SID, #ZONE)
            Check Zone ID (virsh attach) : TIDVMID, TIDSID
            (? detach)


To research: How to remove an iP from either dhcpd or dnsmasq
            How to send vars to Ansible playbook (done)
            Collectd custom plugin (done)
            Grafana custom Dashboard (N/A)
            Decide on collectd custom plugin -> Use the HW script

Plays Needed:
    SR Generation (per tenant)
        # TID
        # Smaller IP address 99.0.0.x/24
        -> Create NS
        -> Add Interface
        -> Create GRE Tunnel
    Subnet Switch (per subnet)
        # TID
        # SID
        # ip addresses
            # Prefix
        -> create ns
        -> Add Interface
        -> Create Bridge
        -> Create virt network
        -> Create VXLAN
        -> Add MASQ in SR
        -> Add GRE Routing rules in SR
    Spawn VM
        # VMID
        # Zone <IP|ID>
        -> clone VM in the correct zone

    Attach Interface
        # VMID
        # SNID
        # IP of VM in that subnet
            # Pull list of all VMs in that subnet from the stored state json
            # add 1 to that.
        -> SSH into Zone
        -> Get the management IP
        -> virsh-attach interface with interface name
        -> Pull the mac for that interface
        -> SSH into the VM
        -> Pull the interface name
        -> Add IP to that interface
